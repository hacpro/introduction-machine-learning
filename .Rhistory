rm(list=ls())
iris_set <- iris[, -5]
head(iris_set)
k_max <- 15 # Mehr als 15 Clusters wollen wir nicht
data <- iris_set
wss <- sapply(1:k_max,
function(k){kmeans(data, k, nstart=10 )$tot.withinss})
plot(1:k_max, wss,
type="b", pch = 19, frame = FALSE,
xlab="Anzahl von Clusters",
ylab="Total Streuung innerhalb Clusters (within-clusters sum ov square")
abline(v = 3, lty =2)
k_max <- 15 # Mehr als 15 Clusters wollen wir nicht
data <- iris_set
wss <- sapply(1:k_max,
function(k){kmeans(data, k, nstart=10 )$tot.withinss})
plot(1:k_max, wss,
type="b", pch = 19, frame = FALSE,
xlab="Anzahl von Clusters",
ylab="Total Streuung innerhalb Clusters (SSE)")
abline(v = 3, lty =2)
k_max <- 15 # Mehr als 15 Clusters wollen wir nicht
wss <- sapply(1:k_max,
function(k){kmeans(iris_set, k, nstart=10 )$tot.withinss})
wss
k_max <- 15 # Mehr als 15 Clusters wollen wir nicht
wss <- sapply(1:k_max,
function(k){kmeans(iris_set, k, nstart=10 )$tot.withinss})
plot(1:k_max, wss,
type="b", pch = 19, frame = FALSE,
xlab="Anzahl von Clusters",
ylab="Total Streuung innerhalb Clusters (SSE)")
abline(v = 3, lty =2)
model <- readRDS("tic-tac-toe.rds")
print(model$Q)
lm_price <- lm(price ~ sqft, buildings)
# File zur Untersuchung laden
buildings <- read.csv2("buildings.csv", sep = ",")
# Eine Uebersicht der Daten  anzeigen
View(buildings)
lm_price <- lm(price ~ sqft, buildings)
attributes(lm_price)
print(lm_price$terms)
# Wir wollen uns als ersten die Preise anschauen
summary(buildings$price)
# Histogramm der Preise ausgeben
hist(buildings$price, breaks = 200,
prob = T)
# Mittelwert und Median darstellen
abline(v = mean(buildings$price),
col = "darkblue",
lwd = 1)
abline(v = median(buildings$price),
col = "darkblue",
lwd = 1,
lty = 2)
par(mfrow=c(1,1))
clear
cls
empty
rm
# Packages referenzieren
library(ggplot2)
library(GGally)
library(plotly)
library(e1071)
library(scatterplot3d)
library(reshape2)
library(caret)
library(rpart)
library(rpart.plot)
library(datasets)
# Allfaellige Variablen aus Umgebung entfernen
rm(list=ls())
# Layout zuruecksetzen
par(mfrow=c(1,1))
# Reproduzierbarkeit sicherstellen
set.seed(1234)
# Arbeitsverzeichnis setzen
setwd("c:/source/introduction-machine-learning")
# File zur Untersuchung laden
buildings <- read.csv2("buildings.csv", sep = ",")
# Eine Uebersicht der Daten  anzeigen
View(buildings)
#
summary(building)
buildings <- read.csv2("buildings.csv", sep = ",")
summary(building)
summary(buildings)
attrbutes(buildings)
columns(buildings)
colnames(buildings)
summarise(buildings)
library(ggplot2)
library(GGally)
library(plotly)
library(e1071)
library(scatterplot3d)
library(reshape2)
library(caret)
library(rpart)
library(rpart.plot)
library(datasets)
# Allfaellige Variablen aus Umgebung entfernen
rm(list=ls())
# Layout zuruecksetzen
par(mfrow=c(1,1))
# Reproduzierbarkeit sicherstellen
set.seed(1234)
# Arbeitsverzeichnis setzen
setwd("c:/source/introduction-machine-learning")
# File zur Untersuchung laden
buildings <- read.csv2("buildings.csv", sep = ",")
# Eine Uebersicht der Daten  anzeigen
View(buildings)
# Wir wollen uns als ersten die Preise anschauen
summary(buildings$price)
# Histogramm der Preise ausgeben
hist(buildings$price, breaks = 200,
prob = T)
# Mittelwert und Median darstellen
abline(v = mean(buildings$price),
col = "darkblue",
lwd = 1)
abline(v = median(buildings$price),
col = "darkblue",
lwd = 1,
lty = 2)
# Kerndichteschätzung einzeichnen
lines(density(buildings$price),
lwd = 1, # thickness of line
col = "red"
)
# Kurtosis und Schiefe ausgeben
skewness(buildings$price)
kurtosis(buildings$price)
lm_price <- lm(price ~ sqft, buildings)
# Modell in Streudiagramm visualisieren
ggplot(buildings, aes(sqft, price)) +
geom_point() +
geom_smooth(method = "lm")
# Kennzahlen ausgeben
summary(lm_price)
lm_price2 <- lm(price ~ sqft + bath, buildings)
summary(lm_price2)
# 3D-Plot anzeigen
plot_ly(buildings, x = ~sqft, y = ~as.integer(bath),
z = ~price,
color = ~price,
type = "scatter3d",
mode = "markers")
predict(lm_price2, data.frame(sqft=2000, bath=as.factor(2)))
# Anzeigen von Erhoehung, gruppiert nach Stadt
df <- buildings
df$in_sf <- as.factor(buildings[, 1])
ggplot(df, aes(in_sf, elevation, colour = in_sf)) +
geom_point(alpha = .2, size = 5)
in_sf <- as.integer(as.logical(buildings$elevation > 73))
# Richtige Kategorisierungen bestimmen
classifications <- df$in_sf == in_sf
all_classifications <- length(classifications)
correct_classifications <- sum(classifications == T)
# Prozent richtige bestimmen
correct_classifications / all_classifications
# Konfusionsmatrix erstellen
confusionMatrix(buildings$in_sf,
in_sf,
dnn = c("Prediction", "Reference"))
# m2-Preis und Hohe darstellen nach Stadt
ggplot(df, aes(price_per_sqft, elevation, colour = in_sf)) +
geom_point(alpha = .5)
# m2-Preis und Hohe darstellen nach Stadt
ggplot(df, aes(price_per_sqft, elevation, colour = in_sf)) +
geom_point(alpha = .5)
# Einzeichnen was wir inzwischen wissen
ggplot(df, aes(price_per_sqft, elevation, colour = in_sf)) +
geom_point(alpha = .5) +
annotate("rect", xmin=0,xmax=Inf, ymin=73, ymax=Inf, alpha=0.2, fill="#00BFC4") +
annotate("rect", xmin=2250,xmax=Inf, ymin=0, ymax=73, alpha=0.2, fill="#F8766D")
ggplot(df, aes(price_per_sqft, elevation, colour = in_sf)) +
geom_point(alpha = .5)
ggplot(df, aes(price_per_sqft, elevation, colour = in_sf)) +
geom_point(alpha = .5) +
annotate("rect", xmin=0,xmax=Inf, ymin=73, ymax=Inf, alpha=0.2, fill="#00BFC4") +
annotate("rect", xmin=2250,xmax=Inf, ymin=0, ymax=73, alpha=0.2, fill="#F8766D")
ggplot(df, aes(price_per_sqft, elevation, colour = in_sf)) +
geom_point(alpha = .5)
annotate("rect", xmin=0,xmax=Inf, ymin=73, ymax=Inf, alpha=0.2, fill="#00BFC4")
ggplot(df, aes(price_per_sqft, elevation, colour = in_sf)) +
geom_point(alpha = .5) +
annotate("rect", xmin=0,xmax=Inf, ymin=73, ymax=Inf, alpha=0.2,
fill="#00BFC4") +
annotate("rect", xmin=2250,xmax=Inf, ymin=0, ymax=73, alpha=0.2,
fill="#F8766D")
df2 <- buildings
df2$in_sf <- as.factor(buildings[, 1])
ggpairs(data = df2, columns = c(4, 5, 6, 7, 8), title = "Korrelationsmatrix",
mapping = ggplot2::aes(colour = in_sf))
fourfoldplot(table(Predicted=as.integer(buildings$elevation > 73),
Actual=buildings$in_sf),
color = c("#CC6666", "#99CC99"),
conf.level = 0, margin = 1,
main = "Confusion Matrix")
fourfoldplot(table(Predicted=as.integer(buildings$elevation > 40),
Actual=buildings$in_sf),
color = c("#CC6666", "#99CC99"),
conf.level = 0, margin = 1,
main = "Confusion Matrix")
# Klassischen Descision Tree erstellen
dtree <- rpart(in_sf ~ ., buildings,
method = "class")
# Anhand von Modell Klassifizierung machen
in_sf_tree <- predict(dtree, buildings, type = "class")
# Schauen wir uns diesen Baum an (Vorteil von Descision Trees)
rpart.plot(dtree)
# Unser Resultat begutachten
confusionMatrix(in_sf_tree,
buildings$in_sf,
dnn = c("Prediction", "Reference"))
in_sf_tree <- predict(dtree, buildings, type = "class")
# Unser Resultat begutachten
confusionMatrix(in_sf_tree,
buildings$in_sf,
dnn = c("Prediction", "Reference"))
# Wir teilen unsere Daten in Training und Testset auf (70%/30%)
train_rows <- sample(nrow(buildings), .7*nrow(buildings))
train_set <- buildings[train_rows,]
validation_set <- buildings[-train_rows,]
# Erstellen das Modell noch einmal neu mit dem Training set
dtree <- rpart(in_sf ~ ., train_set, method = "class")
# Jetzt klassifizieren wir das Test set
in_sf_tree <- predict(dtree, validation_set, type = "class")
# und schauen uns diese Konfusionsmatrix an
confusionMatrix(in_sf_tree,
validation_set$in_sf,
dnn = c("Prediction", "Reference"))
ggpairs(data = iris_set, title = "Korrelationsmatrix")
iris_set <- iris[, -5]
head(iris_set)
ggpairs(data = iris_set, title = "Korrelationsmatrix")
k_max <- 15
wss <- sapply(1:k_max,
function(k){kmeans(iris_set, k, nstart=10 )$tot.withinss})
plot(1:k_max, wss,
type="b", pch = 19, frame = FALSE,
xlab="Anzahl von Clusters",
ylab="Total Streuung innerhalb Clusters (SSE)")
# Berechnen wir die 3 Cluster
clusters <- kmeans(iris_set,
3, # Anzahl erwarteter Cluster
nstart = 20)
# Statistik ausgeben
clusters
# Berechnen wir die 3 Cluster
clusters <- kmeans(iris_set,
3, # Anzahl erwarteter Cluster
nstart = 20)
clusters
# Visualisieren wir unsere Cluster
iris_set$cluster <- as.factor(clusters$cluster)
ggplot(iris_set, aes(Petal.Length, Petal.Width,
color = cluster)) +
geom_point()
ggplot(iris, aes(Petal.Length, Petal.Width,
color = Species)) +
geom_point()
library(recommenderlab)
library(data.table)
library(dplyr)
ratings <- read.csv("movielens-datasets/ratings.csv")
movies_and_genres <- read.csv("movielens-datasets/movies_and_genres.csv",
stringsAsFactors=FALSE)
ratingmat <- dcast(ratings, userId~movieId,
value.var = "rating",
na.rm=FALSE)
ratingmat <- as.matrix(ratingmat[,-1]) # UserId entfernen
ratingmat <- as(ratingmat, "realRatingMatrix")
ratingmat_norm <- normalize(ratingmat)
recommender_model <- Recommender(ratingmat_norm,
method = "UBCF",
param=list(method="Cosine",
nn=30))
recommendations <- predict(recommender_model,
ratingmat[1], n=10)
recommendations_list <- unlist(as(recommendations, "list"))
getMovieTitle <- function(id)
{
movies_and_genres[movies_and_genres$movieId == id, ]$title
}
lapply(recommendations_list, getMovieTitle) %>% unlist
lapply(ratings[ratings$userId == 1 & ratings$rating >= 3.5, ]$movieId,
getMovieTitle) %>% unlist
1 + 1
library(ggplot2)
library(GGally)
library(plotly)
library(e1071)
library(scatterplot3d)
library(reshape2)
library(caret)
library(rpart)
library(rpart.plot)
library(datasets)
rm(list=ls())
# Layout zuruecksetzen
par(mfrow=c(1,1))
# Reproduzierbarkeit sicherstellen
set.seed(1234)
# Arbeitsverzeichnis setzen
setwd("c:/source/introduction-machine-learning")
buildings <- read.csv2("buildings.csv", sep = ",")
# Eine Uebersicht der Daten  anzeigen
View(buildings)
summary(buildings$price)
# Histogramm der Preise ausgeben
hist(buildings$price, breaks = 200,
prob = T)
# Mittelwert und Median darstellen
abline(v = mean(buildings$price),
col = "darkblue",
lwd = 1)
abline(v = median(buildings$price),
col = "darkblue",
lwd = 1,
lty = 2)
# Kerndichteschätzung einzeichnen
lines(density(buildings$price),
lwd = 1, # thickness of line
col = "red"
)
# Kurtosis und Schiefe ausgeben
skewness(buildings$price)
kurtosis(buildings$price)
lm_price <- lm(price ~ sqft, buildings)
# Modell in Streudiagramm visualisieren
ggplot(buildings, aes(sqft, price)) +
geom_point() +
geom_smooth(method = "lm")
# Kennzahlen ausgeben
summary(lm_price)
lm_price2 <- lm(price ~ sqft + bath, buildings)
summary(lm_price2)
# 3D-Plot anzeigen
plot_ly(buildings, x = ~sqft, y = ~as.integer(bath),
z = ~price,
color = ~price,
type = "scatter3d",
mode = "markers")
predict(lm_price2, data.frame(sqft=2000, bath=as.factor(2)))
df <- buildings
df$in_sf <- as.factor(buildings[, 1])
ggplot(df, aes(in_sf, elevation, colour = in_sf)) +
geom_point(alpha = .2, size = 5)
in_sf <- as.integer(as.logical(buildings$elevation > 73))
# Richtige Kategorisierungen bestimmen
classifications <- df$in_sf == in_sf
all_classifications <- length(classifications)
correct_classifications <- sum(classifications == T)
# Prozent richtige bestimmen
correct_classifications / all_classifications
# Konfusionsmatrix erstellen
confusionMatrix(buildings$in_sf,
in_sf,
dnn = c("Prediction", "Reference"))
# Korrelationsmatrix darstellen
# (Wir entfernen kategoriale Variablen mit zu vielen Varianten)
df2 <- buildings
df2$in_sf <- as.factor(buildings[, 1])
ggpairs(data = df2, columns = c(4, 5, 6, 7, 8), title = "Korrelationsmatrix",
mapping = ggplot2::aes(colour = in_sf))
ggplot(df, aes(price_per_sqft, elevation, colour = in_sf)) +
geom_point(alpha = .5)
df2 <- buildings
df2$in_sf <- as.factor(buildings[, 1])
ggpairs(data = df2, columns = c(4, 5, 6, 7, 8), title = "Korrelationsmatrix",
mapping = ggplot2::aes(colour = in_sf))
df2 <- buildings
df2$in_sf <- as.factor(buildings[, 1])
ggpairs(data = df2, columns = c(1,4, 5, 6, 7, 8), title = "Korrelationsmatrix",
mapping = ggplot2::aes(colour = in_sf))
ggplot(df, aes(price_per_sqft, elevation, colour = in_sf)) +
geom_point(alpha = .5)
# Einzeichnen was wir inzwischen wissen
ggplot(df, aes(price_per_sqft, elevation, colour = in_sf)) +
geom_point(alpha = .5) +
annotate("rect", xmin=0,xmax=Inf, ymin=73, ymax=Inf, alpha=0.2,
fill="#00BFC4") +
annotate("rect", xmin=2250,xmax=Inf, ymin=0, ymax=73, alpha=0.2,
fill="#F8766D")
# Schauen wir uns noch einmal unsere erste Regel (> 73ft) an
fourfoldplot(table(Predicted=as.integer(buildings$elevation > 73),
Actual=buildings$in_sf),
color = c("#CC6666", "#99CC99"),
conf.level = 0, margin = 1,
main = "Confusion Matrix")
# Verschieben wir die Hoehengrenze nach unten
fourfoldplot(table(Predicted=as.integer(buildings$elevation > 40),
Actual=buildings$in_sf),
color = c("#CC6666", "#99CC99"),
conf.level = 0, margin = 1,
main = "Confusion Matrix")
# Klassischen Descision Tree erstellen
dtree <- rpart(in_sf ~ ., buildings,
method = "class")
# Schauen wir uns diesen Baum an (Vorteil von Descision Trees)
rpart.plot(dtree)
# Klassischen Descision Tree erstellen
dtree <- rpart(in_sf ~ ., buildings,
method = "class")
# Schauen wir uns diesen Baum an (Vorteil von Descision Trees)
rpart.plot(dtree)
# Anhand von Modell Klassifizierung machen
in_sf_tree <- predict(dtree, buildings, type = "class")
# Unser Resultat begutachten
confusionMatrix(in_sf_tree,
buildings$in_sf,
dnn = c("Prediction", "Reference"))
# Wir teilen unsere Daten in Training und Testset auf (70%/30%)
train_rows <- sample(nrow(buildings), .7*nrow(buildings))
train_set <- buildings[train_rows,]
validation_set <- buildings[-train_rows,]
# Erstellen das Modell noch einmal neu mit dem Training set
dtree <- rpart(in_sf ~ ., train_set, method = "class")
# Jetzt klassifizieren wir das Test set
in_sf_tree <- predict(dtree, validation_set, type = "class")
# und schauen uns diese Konfusionsmatrix an
confusionMatrix(in_sf_tree,
validation_set$in_sf,
dnn = c("Prediction", "Reference"))
iris_set <- iris[, -5]
head(iris_set)
View(iris_set)
ggpairs(data = iris_set, title = "Korrelationsmatrix")
k_max <- 15
wss <- sapply(1:k_max,
function(k){kmeans(iris_set, k, nstart=10 )$tot.withinss})
plot(1:k_max, wss,
type="b", pch = 19, frame = FALSE,
xlab="Anzahl von Clusters",
ylab="Streuung innerhalb Clusters (SSE)")
# Berechnen wir die 3 Cluster
clusters <- kmeans(iris_set,
3, # Anzahl erwarteter Cluster
nstart = 20)
# Statistik ausgeben
clusters
# Visualisieren wir unsere Cluster
iris_set$cluster <- as.factor(clusters$cluster)
ggplot(iris_set, aes(Petal.Length, Petal.Width,
color = cluster)) +
geom_point()
ggplot(iris, aes(Petal.Length, Petal.Width,
color = Species)) +
geom_point()
# AnomalyDetection Paket von Twitter installieren und laden
# devtools::install_github("twitter/AnomalyDetection")
library(AnomalyDetection)
# Schauen wir uns Beispiel-Twitterdaten an
raw_data %>% head()
# Daten laden
data(raw_data)
# Anomaly Detection ausfuehren mit Twitter package
result = AnomalyDetectionTs(raw_data,
max_anoms=0.02,
direction='both',
plot=TRUE)
# Plot anzeigen
result$plot
library(ReinforcementLearning)
# Beispiel Daten laden (100k zufaellig ausgewaelte Tic-Tac-Toe Spiele)
data(tictactoe)
# Jede Zeile zeigt eine Aktion und die Belohnung auf diese Aktion
# (0 = Spiel laeuft weiter, -1 = X verliert, 1 = X gewinnt)
head(tictactoe, 50)
model <- readRDS("tic-tac-toe.rds")
print(model$Q)
View(model$Q)
head(model$Q)
policy(model)["B..BX...X"]
ggplot(buildings, aes(sqft, price)) +
geom_point() +
geom_smooth(method = "lm")
ggplot(buildings, aes(sqft, price)) +
geom_point() +
geom_smooth(method = "smooth")
ggplot(buildings, aes(sqft, price)) +
geom_point() +
geom_smooth(method = "loess")
ggplot(buildings, aes(sqft, price)) +
geom_point() +
geom_smooth(method = "lm") +
geom_smooth(method = "lm") #locally weighted scatterplot smoothing
ggplot(buildings, aes(sqft, price)) +
geom_point() +
geom_smooth(method = "lm") +
geom_smooth(method = "loess") #locally weighted scatterplot smoothing
ggplot(buildings, aes(sqft, price)) +
geom_point() +
#geom_smooth(method = "lm")
geom_smooth(method = "loess") #locally weighted scatterplot smoothing
summary(lm_price)
ggplot(buildings, aes(sqft, price)) +
geom_point() +
#geom_smooth(method = "lm")
geom_smooth(method = "loess") #locally weighted scatterplot smoothing
summary(lm_price)
ggplot(buildings, aes(sqft, price)) +
geom_point() +
geom_smooth(method = "lm")
confusionMatrix(buildings$in_sf,
in_sf,
dnn = c("Prediction", "Reference"))
